{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"jiawei-ucas/ConsistentChat\")[\"train\"]\n",
    "\n",
    "# -------------------------\n",
    "# Pair conversations\n",
    "# -------------------------\n",
    "def to_pairs(conv):\n",
    "    pairs = []\n",
    "    i = 0\n",
    "    while i < len(conv) - 1:\n",
    "        if conv[i][\"from\"] == \"human\" and conv[i+1][\"from\"] == \"gpt\":\n",
    "            pairs.append([conv[i], conv[i+1]])\n",
    "            i += 2\n",
    "        else:\n",
    "            i += 1\n",
    "    return pairs\n",
    "\n",
    "# -------------------------\n",
    "# Merge helpers\n",
    "# -------------------------\n",
    "def merge_pairs(pairs):\n",
    "    return \"\\n\".join([f\"human: {h['value']}\\ngpt: {g['value']}\" for h,g in pairs])\n",
    "\n",
    "def merge_human(pairs):\n",
    "    return \"\\n\".join([f\"human: {h['value']}\" for h,_ in pairs])\n",
    "\n",
    "def merge_gpt(pairs):\n",
    "    return \"\\n\".join([f\"gpt: {g['value']}\" for _,g in pairs])\n",
    "\n",
    "# -------------------------\n",
    "# First + Last\n",
    "# -------------------------\n",
    "def first_last(pairs):\n",
    "    if len(pairs) <= 1:\n",
    "        return pairs\n",
    "    return [pairs[0], pairs[-1]]\n",
    "\n",
    "# -------------------------\n",
    "# Writer\n",
    "# -------------------------\n",
    "paths = {\n",
    "    \"full\": {\n",
    "        \"pair\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/pair_full.jsonl\",\n",
    "        \"human\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/human_full.jsonl\",\n",
    "        \"gpt\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/gpt_full.jsonl\",\n",
    "    },\n",
    "    \"fl\": {\n",
    "        \"pair\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/pair_first_last.jsonl\",\n",
    "        \"human\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/human_first_last.jsonl\",\n",
    "        \"gpt\": \"/home/hyang/BigML_sharding_dataset/Qwen3-VL-Embedding/data/consistent_chat/gpt_first_last.jsonl\",\n",
    "    }\n",
    "}\n",
    "\n",
    "files = {\n",
    "    k: {kk: open(vv, \"w\") for kk, vv in v.items()}\n",
    "    for k, v in paths.items()\n",
    "}\n",
    "\n",
    "# -------------------------\n",
    "# Process\n",
    "# -------------------------\n",
    "for ex in ds:\n",
    "    pairs = to_pairs(ex[\"conversations\"])\n",
    "\n",
    "    # FULL\n",
    "    ex_pair = dict(ex);  ex_pair[\"conversations\"]  = merge_pairs(pairs)\n",
    "    ex_h    = dict(ex);  ex_h[\"conversations\"]     = merge_human(pairs)\n",
    "    ex_g    = dict(ex);  ex_g[\"conversations\"]     = merge_gpt(pairs)\n",
    "\n",
    "    files[\"full\"][\"pair\"].write(json.dumps(ex_pair)+\"\\n\")\n",
    "    files[\"full\"][\"human\"].write(json.dumps(ex_h)+\"\\n\")\n",
    "    files[\"full\"][\"gpt\"].write(json.dumps(ex_g)+\"\\n\")\n",
    "\n",
    "    # FIRST LAST\n",
    "    fl = first_last(pairs)\n",
    "\n",
    "    ex_pair = dict(ex);  ex_pair[\"conversations\"]  = merge_pairs(fl)\n",
    "    ex_h    = dict(ex);  ex_h[\"conversations\"]     = merge_human(fl)\n",
    "    ex_g    = dict(ex);  ex_g[\"conversations\"]     = merge_gpt(fl)\n",
    "\n",
    "    files[\"fl\"][\"pair\"].write(json.dumps(ex_pair)+\"\\n\")\n",
    "    files[\"fl\"][\"human\"].write(json.dumps(ex_h)+\"\\n\")\n",
    "    files[\"fl\"][\"gpt\"].write(json.dumps(ex_g)+\"\\n\")\n",
    "\n",
    "# close\n",
    "for group in files.values():\n",
    "    for f in group.values():\n",
    "        f.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
